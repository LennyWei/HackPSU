# GestureAI
This is our submittion for the **2025 Spring HackPSU**

## What is it?
We've created a program ran on a localhost website that allows the user to utilize the Mediapipe hand landmark detection API to create a gesture dataset themselves. Then they  train the model using Tensorflow and bind their gestures to keyboard & mouse inputs, after which they can use the testing mode finally use the model. 

### Training Recommendations
We recommend to at least train around 50-100 images for each gesture, making sure to include the other hand as well as different angles. The amount of images for each gesture should also be around the same number. 
